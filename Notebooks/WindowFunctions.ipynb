{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Analytical Window Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\n",
    "\n",
    "https://alvinhenrick.com/2017/05/16/apache-spark-analytical-window-functions/\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salary within each department\n",
    "\n",
    "* ### Ranking functions\n",
    "    * rank\n",
    "    * dense_rank\n",
    "    * row_number\n",
    "    * percent_rank?\n",
    "    * ntile\n",
    "* ### Analytic functions\n",
    "    * lead\n",
    "    * lag\n",
    "    * cume_dist(cumulative distribution)?\n",
    "* ### Aggregate functions\n",
    "    * sum\n",
    "    * running sum\n",
    "    * avg\n",
    "    * max\n",
    "    * min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Spark by creating a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"WindowFunctions\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+---------+----+----+------+\n",
      "|empno| ename|      job| mgr| hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+---------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|17-Dec-80| 800|  20|    10|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-Feb-81|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|22-Feb-81|1250| 500|    30|\n",
      "| 7522|   KIM| SALESMAN|7698|25-Feb-81|1200| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839| 2-Apr-81|2975|   0|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|28-Sep-81|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839| 1-May-81|2850|   0|    30|\n",
      "| 7782| CLARK|  MANAGER|7839| 9-Jun-81|2450|   0|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-Apr-87|3000|   0|    20|\n",
      "| 7839|  KING|PRESIDENT|   0|17-Nov-81|5000|   0|    10|\n",
      "| 7844|TURNER| SALESMAN|7698| 8-Sep-81|1500|   0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|23-May-87|1100|   0|    20|\n",
      "+-----+------+---------+----+---------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "#Create Sample Dataframe\n",
    "empDF = [\n",
    "      (7369, \"SMITH\", \"CLERK\", 7902, \"17-Dec-80\", 800, 20, 10),\n",
    "      (7499, \"ALLEN\", \"SALESMAN\", 7698, \"20-Feb-81\", 1600, 300, 30),\n",
    "      (7521, \"WARD\", \"SALESMAN\", 7698, \"22-Feb-81\", 1250, 500, 30),\n",
    "      (7522, \"KIM\", \"SALESMAN\", 7698, \"25-Feb-81\", 1200, 500, 30),\n",
    "      (7566, \"JONES\", \"MANAGER\", 7839, \"2-Apr-81\", 2975, 0, 20),\n",
    "      (7654, \"MARTIN\", \"SALESMAN\", 7698, \"28-Sep-81\", 1250, 1400, 30),\n",
    "      (7698, \"BLAKE\", \"MANAGER\", 7839, \"1-May-81\", 2850, 0, 30),\n",
    "      (7782, \"CLARK\", \"MANAGER\", 7839, \"9-Jun-81\", 2450, 0, 10),\n",
    "      (7788, \"SCOTT\", \"ANALYST\", 7566, \"19-Apr-87\", 3000, 0, 20),\n",
    "      (7839, \"KING\", \"PRESIDENT\", 0, \"17-Nov-81\", 5000, 0, 10),\n",
    "      (7844, \"TURNER\", \"SALESMAN\", 7698, \"8-Sep-81\", 1500, 0, 30),\n",
    "      (7876, \"ADAMS\", \"CLERK\", 7788, \"23-May-87\", 1100, 0, 20)]\n",
    "\n",
    "empDF = spark.createDataFrame(empDF, [\"empno\", \"ename\", \"job\", \"mgr\", \"hiredate\", \"sal\", \"comm\", \"deptno\"])\n",
    "    \n",
    "empDF.show()   \n",
    "\n",
    "empDF.registerTempTable(\"emp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking functions\n",
    "\n",
    "#### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+----+----------+----------+------------+-----+-------------------+\n",
      "|empno|deptno| sal|rank|dense_rank|row_number|percent_rank|ntile|          cume_dist|\n",
      "+-----+------+----+----+----------+----------+------------+-----+-------------------+\n",
      "| 7839|    10|5000|   1|         1|         1|         0.0|    1| 0.3333333333333333|\n",
      "| 7782|    10|2450|   2|         2|         2|         0.5|    2| 0.6666666666666666|\n",
      "| 7369|    10| 800|   3|         3|         3|         1.0|    3|                1.0|\n",
      "| 7698|    30|2850|   1|         1|         1|         0.0|    1|0.16666666666666666|\n",
      "| 7499|    30|1600|   2|         2|         2|         0.2|    1| 0.3333333333333333|\n",
      "| 7844|    30|1500|   3|         3|         3|         0.4|    2|                0.5|\n",
      "| 7521|    30|1250|   4|         4|         4|         0.6|    2| 0.8333333333333334|\n",
      "| 7654|    30|1250|   4|         4|         5|         0.6|    3| 0.8333333333333334|\n",
      "| 7522|    30|1200|   6|         5|         6|         1.0|    3|                1.0|\n",
      "| 7788|    20|3000|   1|         1|         1|         0.0|    1| 0.3333333333333333|\n",
      "| 7566|    20|2975|   2|         2|         2|         0.5|    2| 0.6666666666666666|\n",
      "| 7876|    20|1100|   3|         3|         3|         1.0|    3|                1.0|\n",
      "+-----+------+----+----+----------+----------+------------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ranking = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    empno,\n",
    "    deptno,\n",
    "    sal,\n",
    "    RANK() OVER (partition by deptno ORDER BY sal desc) as rank, \n",
    "    DENSE_RANK() OVER (partition by deptno ORDER BY sal desc) as dense_rank,\n",
    "    ROW_NUMBER() OVER (partition by deptno ORDER BY sal desc) as row_number,\n",
    "    PERCENT_RANK() OVER (partition by deptno ORDER BY sal desc) as percent_rank,\n",
    "    NTILE(3) OVER (partition by deptno ORDER BY sal desc) as ntile,\n",
    "    CUME_DIST() OVER (PARTITION BY deptno ORDER BY sal desc) as cume_dist\n",
    "    FROM emp\n",
    "\"\"\")\n",
    "\n",
    "ranking.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DF API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+----+----------+-------+-----------+--------+--------+\n",
      "|empno|deptno| sal|rank|dense_rank|row_num|running_sum|next_val|prev_val|\n",
      "+-----+------+----+----+----------+-------+-----------+--------+--------+\n",
      "| 7839|    10|5000|   1|         1|      1|       5000|    2450|    null|\n",
      "| 7782|    10|2450|   2|         2|      2|       7450|     800|    5000|\n",
      "| 7369|    10| 800|   3|         3|      3|       8250|    null|    2450|\n",
      "| 7698|    30|2850|   1|         1|      1|       2850|    1600|    null|\n",
      "| 7499|    30|1600|   2|         2|      2|       4450|    1500|    2850|\n",
      "| 7844|    30|1500|   3|         3|      3|       5950|    1250|    1600|\n",
      "| 7521|    30|1250|   4|         4|      4|       8450|    1250|    1500|\n",
      "| 7654|    30|1250|   4|         4|      5|       8450|    1200|    1250|\n",
      "| 7522|    30|1200|   6|         5|      6|       9650|    null|    1250|\n",
      "| 7788|    20|3000|   1|         1|      1|       3000|    2975|    null|\n",
      "| 7566|    20|2975|   2|         2|      2|       5975|    1100|    3000|\n",
      "| 7876|    20|1100|   3|         3|      3|       7075|    null|    2975|\n",
      "+-----+------+----+----+----------+-------+-----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(empDF['deptno']).orderBy(empDF['sal'].desc())\n",
    "rank = func.rank().over(windowSpec)\n",
    "denseRank = func.dense_rank().over(windowSpec)\n",
    "rowNum = func.row_number().over(windowSpec)\n",
    "runningSum = func.sum(empDF['sal']).over(windowSpec)\n",
    "lead = func.lead(empDF['sal']).over(windowSpec)\n",
    "lag = func.lag(empDF['sal']).over(windowSpec)\n",
    "empDF.select('empno','deptno','sal',\n",
    "             rank.alias('rank'),\n",
    "             denseRank.alias('dense_rank'),\n",
    "             rowNum.alias('row_num'),\n",
    "             runningSum.alias('running_sum'),\n",
    "             lead.alias('next_val'),\n",
    "             lag.alias('prev_val')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic functions\n",
    "\n",
    "#### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+--------+----------------+--------+-----+----+\n",
      "|empno|deptno| sal|next_val|next_to_next_val|prev_val|first|last|\n",
      "+-----+------+----+--------+----------------+--------+-----+----+\n",
      "| 7839|    10|5000|    2450|             800|    null| 5000| 800|\n",
      "| 7782|    10|2450|     800|            null|    5000| 5000| 800|\n",
      "| 7369|    10| 800|    null|            null|    2450| 5000| 800|\n",
      "| 7698|    30|2850|    1600|            1500|    null| 2850|1200|\n",
      "| 7499|    30|1600|    1500|            1250|    2850| 2850|1200|\n",
      "| 7844|    30|1500|    1250|            1250|    1600| 2850|1200|\n",
      "| 7521|    30|1250|    1250|            1200|    1500| 2850|1200|\n",
      "| 7654|    30|1250|    1200|            null|    1250| 2850|1200|\n",
      "| 7522|    30|1200|    null|            null|    1250| 2850|1200|\n",
      "| 7788|    20|3000|    2975|            1100|    null| 3000|1100|\n",
      "| 7566|    20|2975|    1100|            null|    3000| 3000|1100|\n",
      "| 7876|    20|1100|    null|            null|    2975| 3000|1100|\n",
      "+-----+------+----+--------+----------------+--------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rankTest = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    empno,\n",
    "    deptno,\n",
    "    sal,\n",
    "    lead(sal) OVER (PARTITION BY deptno ORDER BY sal desc) as next_val,\n",
    "    lead(sal,2) OVER (PARTITION BY deptno ORDER BY sal desc) as next_to_next_val,\n",
    "    lag(sal) OVER (PARTITION BY deptno ORDER BY sal desc) as prev_val,\n",
    "    first(sal) OVER (PARTITION BY deptno ORDER BY sal desc) as first,\n",
    "    last(sal) OVER (PARTITION BY deptno ORDER BY sal desc ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) as last\n",
    "    FROM emp\n",
    "\"\"\")\n",
    "\n",
    "rankTest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+-----------+----+------------------+----+----+-----+\n",
      "|empno|deptno| sal|running_sum| sum|               avg| max| min|count|\n",
      "+-----+------+----+-----------+----+------------------+----+----+-----+\n",
      "| 7839|    10|5000|       5000|8250|            2750.0|5000| 800|    3|\n",
      "| 7782|    10|2450|       7450|8250|            2750.0|5000| 800|    3|\n",
      "| 7369|    10| 800|       8250|8250|            2750.0|5000| 800|    3|\n",
      "| 7698|    30|2850|       2850|9650|1608.3333333333333|2850|1200|    6|\n",
      "| 7499|    30|1600|       4450|9650|1608.3333333333333|2850|1200|    6|\n",
      "| 7844|    30|1500|       5950|9650|1608.3333333333333|2850|1200|    6|\n",
      "| 7521|    30|1250|       8450|9650|1608.3333333333333|2850|1200|    6|\n",
      "| 7654|    30|1250|       8450|9650|1608.3333333333333|2850|1200|    6|\n",
      "| 7522|    30|1200|       9650|9650|1608.3333333333333|2850|1200|    6|\n",
      "| 7788|    20|3000|       3000|7075|2358.3333333333335|3000|1100|    3|\n",
      "| 7566|    20|2975|       5975|7075|2358.3333333333335|3000|1100|    3|\n",
      "| 7876|    20|1100|       7075|7075|2358.3333333333335|3000|1100|    3|\n",
      "+-----+------+----+-----------+----+------------------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rankTest = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    empno,\n",
    "    deptno,\n",
    "    sal,\n",
    "    sum(sal) OVER (PARTITION BY deptno ORDER BY sal desc) as running_sum,\n",
    "    sum(sal) OVER (PARTITION BY deptno) as sum,\n",
    "    avg(sal) OVER (PARTITION BY deptno) as avg,\n",
    "    max(sal) OVER (PARTITION BY deptno) as max,\n",
    "    min(sal) OVER (PARTITION BY deptno) as min,\n",
    "    count(*) OVER (PARTITION BY deptno) as count\n",
    "    FROM emp\n",
    "\"\"\")\n",
    "\n",
    "rankTest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \\\n",
    "  [(\"Thin\", \"Cell Phone\", 6000),\n",
    "  (\"Normal\", \"Tablet\", 1500),\n",
    "  (\"Mini\", \"Tablet\", 5500),\n",
    "  (\"Ultra thin\", \"Cell Phone\", 5500),\n",
    "  (\"Very thin\", \"Cell Phone\", 6000),\n",
    "  (\"Big\", \"Tablet\", 2500),\n",
    "  (\"Bendable\", \"Cell Phone\", 3000),\n",
    "  (\"Foldable\", \"Cell Phone\", 3000),\n",
    "  (\"Pro\", \"Tablet\", 4500),\n",
    "  (\"Pro2\", \"Tablet\", 6500)]\n",
    "df = spark.createDataFrame(data, [\"product\", \"category\", \"revenue\"])\n",
    "df.registerTempTable(\"productRevenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is the difference between the revenue of a product and the revenue of the best selling product in the same category as this product?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|   product|  category|revenue|\n",
      "+----------+----------+-------+\n",
      "|      Pro2|    Tablet|   6500|\n",
      "|      Mini|    Tablet|   5500|\n",
      "|      Thin|Cell Phone|   6000|\n",
      "| Very thin|Cell Phone|   6000|\n",
      "|Ultra thin|Cell Phone|   5500|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  product,\n",
    "  category,\n",
    "  revenue\n",
    "FROM (\n",
    "  SELECT\n",
    "    product,\n",
    "    category,\n",
    "    revenue,\n",
    "    dense_rank() OVER (PARTITION BY category ORDER BY revenue DESC) as rank\n",
    "  FROM productRevenue) tmp\n",
    "WHERE\n",
    "  rank <= 2\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "#Check the Query Plan\n",
    "#df.explain(extended = True)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Calculate the MAX reveue per catagory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+----+\n",
      "|   product|  category|revenue| max|\n",
      "+----------+----------+-------+----+\n",
      "|    Normal|    Tablet|   1500|6500|\n",
      "|      Mini|    Tablet|   5500|6500|\n",
      "|       Big|    Tablet|   2500|6500|\n",
      "|       Pro|    Tablet|   4500|6500|\n",
      "|      Pro2|    Tablet|   6500|6500|\n",
      "|      Thin|Cell Phone|   6000|6000|\n",
      "|Ultra thin|Cell Phone|   5500|6000|\n",
      "| Very thin|Cell Phone|   6000|6000|\n",
      "|  Bendable|Cell Phone|   3000|6000|\n",
      "|  Foldable|Cell Phone|   3000|6000|\n",
      "+----------+----------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "  SELECT\n",
    "    product,\n",
    "    category,\n",
    "    revenue,\n",
    "    max(revenue) OVER (PARTITION BY category) as max\n",
    "  FROM productRevenue\n",
    "\"\"\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What are the best selling and second best selling products in every category?\n",
    "Below is the SQL query used to answer this question by using window function dense_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+------------------+\n",
      "|   product|  category|revenue|revenue_difference|\n",
      "+----------+----------+-------+------------------+\n",
      "|      Pro2|    Tablet|   6500|                 0|\n",
      "|      Mini|    Tablet|   5500|              1000|\n",
      "|       Pro|    Tablet|   4500|              2000|\n",
      "|       Big|    Tablet|   2500|              4000|\n",
      "|    Normal|    Tablet|   1500|              5000|\n",
      "|      Thin|Cell Phone|   6000|                 0|\n",
      "| Very thin|Cell Phone|   6000|                 0|\n",
      "|Ultra thin|Cell Phone|   5500|               500|\n",
      "|  Bendable|Cell Phone|   3000|              3000|\n",
      "|  Foldable|Cell Phone|   3000|              3000|\n",
      "+----------+----------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window function partioned by Category and ordered by Revenue\n",
    "windowSpec = \\\n",
    "  Window \\\n",
    "    .partitionBy(df['category']) \\\n",
    "    .orderBy(df['revenue'].desc()) \\\n",
    "    .rangeBetween(-sys.maxsize, sys.maxsize)\n",
    "    \n",
    "# Create dataframe based on the productRevenue table    \n",
    "dataFrame = spark.table(\"productRevenue\")\n",
    "\n",
    "# Calculate the Revenue difference\n",
    "revenue_difference = \\\n",
    "  (func.max(dataFrame['revenue']).over(windowSpec) - dataFrame['revenue'])\n",
    "  \n",
    "#Generate a new dataframe (original dataframe and the revenue difference)\n",
    "revenue_diff = dataFrame.select(\n",
    "  dataFrame['product'],\n",
    "  dataFrame['category'],\n",
    "  dataFrame['revenue'],\n",
    "  revenue_difference.alias(\"revenue_difference\"))\n",
    "\n",
    "revenue_diff.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
