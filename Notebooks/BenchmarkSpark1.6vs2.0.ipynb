{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Spark 2.0's Tungsten engine\n",
    "### \"whole-stage code generation\"\n",
    "\n",
    "This notebook demonstrates the power of whole-stage code generation, a technique that blends state-of-the-art from modern compilers and MPP databases. In order to compare the performance with Spark 1.6, we turn off whole-stage code generation in Spark 2.0, which would result in using a similar code path as in Spark 1.6.\n",
    "\n",
    "To read the companion blog posts, click the following:\n",
    "- https://databricks.com/blog/2016/05/11/spark-2-0-technical-preview-easier-faster-and-smarter.html\n",
    "- https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Spark by creating a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Benchmark\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def benchmark(name,MenthodToRun):\n",
    "    startTime = time()\n",
    "    MenthodToRun\n",
    "    TotalTime = time() - startTime\n",
    "    print(\"Time taken in {}: {} seconds\".format(name, TotalTime))\n",
    "    del startTime\n",
    "    del TotalTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How fast can Spark 1.6 vs 2.0 sum up 1 billion numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|           sum(id)|\n",
      "+------------------+\n",
      "|499999999500000000|\n",
      "+------------------+\n",
      "\n",
      "Sum done in 11.55 seconds without wholeStage CodeGen\n",
      "+------------------+\n",
      "|           sum(id)|\n",
      "+------------------+\n",
      "|499999999500000000|\n",
      "+------------------+\n",
      "\n",
      "Sum done in 0.431 seconds with wholeStage CodeGen\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "#This config turns off whole stage code generation, effectively changing the execution path to be similar to Spark 1.6.\n",
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", False)\n",
    "\n",
    "#benchmark(\"Spark 1.6\",spark.range(1000L * 1000 * 1000).selectExpr(\"sum(id)\").show())\n",
    "\n",
    "t0 = time()\n",
    "spark.range(1000L * 1000 * 1000).selectExpr(\"sum(id)\").show()\n",
    "tt = time() - t0\n",
    "\n",
    "print(\"Sum done in {} seconds without wholeStage CodeGen\".format(round(tt,3)))\n",
    "\n",
    "#This config turns on whole stage code generation, effectively changing the execution path to be similar to Spark 2.0.\n",
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", True)\n",
    "\n",
    "t0 = time()\n",
    "spark.range(1000L * 1000 * 1000).selectExpr(\"sum(id)\").show()\n",
    "tt = time() - t0\n",
    "\n",
    "print(\"Sum done in {} seconds with wholeStage CodeGen\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Other primitive operations\n",
    "\n",
    "Databricks has benchmarked the efficiency of other primitive operations under whole-stage code generation. The table below summarizes the result:\n",
    "\n",
    "The way we benchmark is to to measure the cost per row, in nanoseconds.\n",
    "\n",
    "Runtime: Intel Haswell i7 4960HQ 2.6GHz, HotSpot 1.8.0_60-b27, Mac OS X 10.11\n",
    "\n",
    "|                       | Spark 1.6 | Spark 2.0 |\n",
    "|:---------------------:|:---------:|:---------:|\n",
    "|         filter        |   15 ns   |   1.1 ns  |\n",
    "|     sum w/o group     |   14 ns   |   0.9 ns  |\n",
    "|      sum w/ group     |   79 ns   |  10.7 ns  |\n",
    "|       hash join       |   115 ns  |   4.0 ns  |\n",
    "|  sort (8 bit entropy) |   620 ns  |   5.3 ns  |\n",
    "| sort (64 bit entropy) |   620 ns  |   40 ns   |\n",
    "|    sort-merge join    |   750 ns  |   700 ns  |\n",
    "\n",
    "\n",
    "Again, to read the companion blog post, click here: https://databricks.com/blog/2016/05/11/spark-2-0-technical-preview-easier-faster-and-smarter.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How fast can Spark 1.6 vs 2.0 join 1 billion records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join completed in 34.575 seconds without wholeStage CodeGen\n",
      "Join completed in 0.425 seconds with wholeStage CodeGen\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", False)\n",
    "\n",
    "t0 = time()\n",
    "spark.range(1000L * 1000 * 1000).join(spark.range(1000L).toDF(\"id\"), \"id\").count()\n",
    "tt = time() - t0\n",
    "\n",
    "print(\"Join completed in {} seconds without wholeStage CodeGen\".format(round(tt,3)))\n",
    "\n",
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", True)\n",
    "\n",
    "t0 = time()\n",
    "spark.range(1000L * 1000 * 1000).join(spark.range(1000L).toDF(\"id\"), \"id\").count()\n",
    "tt = time() - t0\n",
    "\n",
    "print(\"Join completed in {} seconds with wholeStage CodeGen\".format(round(tt,3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the execution plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[], functions=[sum(id#1078L)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *HashAggregate(keys=[], functions=[partial_sum(id#1078L)])\n",
      "      +- *Filter (id#1078L > 100)\n",
      "         +- *Range (0, 1000, step=1, splits=4)\n"
     ]
    }
   ],
   "source": [
    "spark.range(1000).filter(\"id > 100\").selectExpr(\"sum(id)\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explain function has been extended for whole-stage code generation. When an operator has a star around it (*), whole-stage code generation is enabled. In the following case, Range, Filter, and the two Aggregates are both running with whole-stage code generation. Exchange does not have whole-stage code generation because it is sending data across the network.\n",
    "\n",
    "This query plan has two \"stages\" (divided by Exchange). In the first stage, three operators (Range, Filter, Aggregate) are collapsed into a single function. In the second stage, there is only a single operator (Aggregate).\n",
    "\n",
    "Reference : Databricks Scala Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
