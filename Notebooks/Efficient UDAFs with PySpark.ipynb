{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UD(A)Fs with PySpark\n",
    "Ref : https://www.inovex.de/blog/efficient-udafs-with-pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Spark by creating a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"UD(A)F\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def rows_to_pandas(rows):\n",
    "    \"\"\"Converts a Spark Row iterator of a partition to a Pandas DataFrame assuming YARN\n",
    "\n",
    "    Args:\n",
    "        rows: iterator over PySpark Row objects\n",
    "\n",
    "    Returns:\n",
    "        Pandas DataFrame\n",
    "    \"\"\"\n",
    "    first_row, rows = peek(rows)\n",
    "    if not first_row:\n",
    "        _logger.warning(\"Spark DataFrame is empty! Returning empty Pandas DataFrame!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    first_row_info = [\"{} ({}): {}\".format(k, rtype(first_row[k]), first_row[k])\n",
    "                      for k in first_row.__fields__]\n",
    "    _logger.debug(\"First partition row: {}\".format(first_row_info))\n",
    "    df = pd.DataFrame.from_records(rows, columns=first_row.__fields__)\n",
    "    _logger.debug(\"Converted partition to DataFrame of shape {} with types:\\n{}\".format(df.shape, df.dtypes))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def peek(iterable):\n",
    "    \"\"\"Peek into the first element and return the whole iterator again\n",
    "\n",
    "    Args:\n",
    "        iterable: iterable object like list or iterator\n",
    "\n",
    "    Returns:\n",
    "        tuple of first element and original iterable\n",
    "    \"\"\"\n",
    "    iterable = iter(iterable)\n",
    "    try:\n",
    "        first_elem = next(iterable)\n",
    "    except StopIteration:\n",
    "        return None, iterable\n",
    "    iterable = chain([first_elem], iterable)\n",
    "    return first_elem, iterable\n",
    "\n",
    "\n",
    "def rtype(var):\n",
    "    \"\"\"Heuristic representation for nested types/containers\n",
    "\n",
    "    Args:\n",
    "        var: some (nested) variable\n",
    "\n",
    "    Returns:\n",
    "        str: string representation of nested datatype (NA=Not Available)\n",
    "    \"\"\"\n",
    "    def etype(x):\n",
    "        return type(x).__name__\n",
    "\n",
    "    if isinstance(var, list):\n",
    "        elem_type = etype(var[0]) if var else \"NA\"\n",
    "        return \"List[{}]\".format(elem_type)\n",
    "    elif isinstance(var, dict):\n",
    "        keys = list(var.keys())\n",
    "        if keys:\n",
    "            key = keys[0]\n",
    "            key_type, val_type = etype(key), etype(var[key])\n",
    "        else:\n",
    "            key_type, val_type = \"NA\", \"NA\"\n",
    "        return \"Dict[{}, {}]\".format(key_type, val_type)\n",
    "    elif isinstance(var, tuple):\n",
    "        elem_types = ', '.join(etype(elem) for elem in var)\n",
    "        return \"Tuple[{}]\".format(elem_types)\n",
    "    else:\n",
    "        return etype(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "\n",
    "def convert_dtypes(rows):\n",
    "    \"\"\"Converts some Pandas data types to pure Python data types\n",
    "\n",
    "    Args:\n",
    "        rows (array): numpy recarray holding all rows\n",
    "\n",
    "    Returns:\n",
    "        Iterator over lists of row values\n",
    "    \"\"\"\n",
    "    dtype_map = {pd.Timestamp: lambda x: x.to_pydatetime(),\n",
    "                 np.int8: lambda x: int(x),\n",
    "                 np.int16: lambda x: int(x),\n",
    "                 np.int32: lambda x: int(x),\n",
    "                 np.int64: lambda x: int(x),\n",
    "                 np.float16: lambda x: float(x),\n",
    "                 np.float32: lambda x: float(x),\n",
    "                 np.float64: lambda x: float(x),\n",
    "                 np.float128: lambda x: float(x)}\n",
    "    for row in rows:\n",
    "        yield [dtype_map.get(type(elem), lambda x: x)(elem) for elem in row]\n",
    "\n",
    "\n",
    "def pandas_to_rows(df):\n",
    "    \"\"\"Converts Pandas DataFrame to iterator of Row objects\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        Iterator over PySpark Row objects\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        _logger.debug(\"Returning nothing\")\n",
    "        return iter([])\n",
    "    if type(df) is pd.Series:\n",
    "        df = df.to_frame().T\n",
    "    if df.empty:\n",
    "        _logger.warning(\"Pandas DataFrame is empty! Returning nothing!\")\n",
    "        return iter([])\n",
    "    _logger.debug(\"Convert DataFrame of shape {} to partition with types:\\n{}\".format(df.shape, df.dtypes))\n",
    "    records = df.to_records(index=False)\n",
    "    records = convert_dtypes(records)\n",
    "    first_row, records = peek(records)\n",
    "    first_row_info = [\"{} ({}): {}\".format(k, rtype(v), v) for k, v in zip(df.columns, first_row)]\n",
    "    _logger.debug(\"First record row: {}\".format(first_row_info))\n",
    "    row = Row(*df.columns)\n",
    "    return (row(*elems) for elems in records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "\n",
    "class pandas_udaf(object):\n",
    "    \"\"\"Decorator for PySpark UDAFs using Pandas\n",
    "\n",
    "    Args:\n",
    "        loglevel (int): minimum loglevel for emitting messages\n",
    "    \"\"\"\n",
    "    def __init__(self, loglevel=logging.INFO):\n",
    "        self.loglevel = loglevel\n",
    "\n",
    "    def __call__(self, func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args):\n",
    "            # use *args to allow decorating methods (incl. self arg)\n",
    "            args = list(args)\n",
    "            setup_logger(loglevel=self.loglevel)\n",
    "            args[-1] = rows_to_pandas(args[-1])\n",
    "            df = func(*args)\n",
    "            return pandas_to_rows(df)\n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def setup_logger(loglevel=logging.INFO, logfile=\"pyspark.log\"):\n",
    "    \"\"\"Setup basic logging for logging on the executor\n",
    "\n",
    "    Args:\n",
    "        loglevel (int): minimum loglevel for emitting messages\n",
    "        logfile (str): name of the logfile\n",
    "    \"\"\"\n",
    "    logformat = \"%(asctime)s %(levelname)s %(module)s.%(funcName)s: %(message)s\"\n",
    "    datefmt = \"%y/%m/%d %H:%M:%S\"\n",
    "    try:\n",
    "        logfile = os.path.join(os.environ['LOG_DIRS'].split(',')[0], logfile)\n",
    "    except (KeyError, IndexError):\n",
    "        logging.basicConfig(level=loglevel,\n",
    "                            stream=sys.stdout, \n",
    "                            format=logformat,\n",
    "                            datefmt=datefmt)\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.error(\"LOG_DIRS is not in environment variables or empty, using STDOUT instead.\")\n",
    "\n",
    "    logging.basicConfig(level=loglevel,\n",
    "                        filename=logfile,\n",
    "                        format=logformat,\n",
    "                        datefmt=datefmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark_udaf\n",
    "import logging\n",
    "\n",
    "\n",
    "@pyspark_udaf.pandas_udaf(loglevel=logging.DEBUG)\n",
    "def my_func(df):\n",
    "    if df.empty:\n",
    "        return\n",
    "    df = df.groupby('country').apply(lambda x: x.drop('country', axis=1).describe())\n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   country level_1  feature1  feature2\n",
      "0      FRA   count  3.000000  3.000000\n",
      "1      FRA    mean  1.000000  5.666667\n",
      "2      FRA     std  1.000000  2.516611\n",
      "3      FRA     min  0.000000  3.000000\n",
      "4      FRA     25%  0.500000  4.500000\n",
      "5      FRA     50%  1.000000  6.000000\n",
      "6      FRA     75%  1.500000  7.000000\n",
      "7      FRA     max  2.000000  8.000000\n",
      "8      DEU   count  3.000000  3.000000\n",
      "9      DEU    mean  2.666667  5.666667\n",
      "10     DEU     std  0.577350  4.041452\n",
      "11     DEU     min  2.000000  1.000000\n",
      "12     DEU     25%  2.500000  4.500000\n",
      "13     DEU     50%  3.000000  8.000000\n",
      "14     DEU     75%  3.000000  8.000000\n",
      "15     DEU     max  3.000000  8.000000\n"
     ]
    }
   ],
   "source": [
    "# make pyspark_udaf.py available to the executors\n",
    "#spark.sparkContext.addFile('./pyspark_udaf.py')\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    data = [('DEU', 2, 1.0), ('DEU', 3, 8.0), ('FRA', 2, 6.0),\n",
    "            ('FRA', 0, 8.0), ('DEU', 3, 8.0), ('FRA', 1, 3.0)],\n",
    "    schema = ['country', 'feature1', 'feature2'])\n",
    "\n",
    "stats_df = df.repartition('country').rdd.mapPartitions(my_func).toDF()\n",
    "print(stats_df.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('F1','A1',30,800),\n",
    "('F1','A2',60,100),\n",
    "('F1','A3',90,150),\n",
    "('F2','B1',30,50),\n",
    "('F2','B2',60,40),\n",
    "('F2','B3',90,60),\n",
    "('F2','B4',0,200),\n",
    "('F3','C1',30,90),\n",
    "('F3','C2',60,50),\n",
    "('F3','C3',90,10),\n",
    "('F4','D1',30,300),\n",
    "('F4','D2',0,20),\n",
    "('F4','D3',90,100),\n",
    "('F4','D4',0,60)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"facility\", \"account\", \"delq\",\"bal\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----+---+\n",
      "|facility|account|delq|bal|\n",
      "+--------+-------+----+---+\n",
      "|      F1|     A1|  30|800|\n",
      "|      F1|     A2|  60|100|\n",
      "|      F1|     A3|  90|150|\n",
      "|      F2|     B1|  30| 50|\n",
      "|      F2|     B2|  60| 40|\n",
      "|      F2|     B3|  90| 60|\n",
      "|      F2|     B4|   0|200|\n",
      "|      F3|     C1|  30| 90|\n",
      "|      F3|     C2|  60| 50|\n",
      "|      F3|     C3|  90| 10|\n",
      "|      F4|     D1|  30|300|\n",
      "|      F4|     D2|   0| 20|\n",
      "|      F4|     D3|  90|100|\n",
      "|      F4|     D4|   0| 60|\n",
      "+--------+-------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark_udaf\n",
    "import logging\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "@pyspark_udaf.pandas_udaf(loglevel=logging.DEBUG)\n",
    "def my_udf(df):\n",
    "    if df.empty:\n",
    "        return\n",
    "    df['max'] = df.bal.max()\n",
    "    grouped_df = df.groupby('facility').max()['bal']\n",
    "    df = df.drop(['bal'], axis=1)\n",
    "    final = df.join(grouped_df , on = 'facility')\n",
    "    final['bal_10'] = final['bal'] * 10\n",
    "    return final.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.rdd.getNumPartitions())\n",
    "df = df.withColumn(\"sim_id\", lit(0))\n",
    "df_all = df\n",
    "N = 400\n",
    "\n",
    "for i in range(1,N):\n",
    "    new_df = df.withColumn(\"sim_id\", lit(i))\n",
    "    df_all = df_all.union(new_df)\n",
    "\n",
    "#df_all.show()\n",
    "print(df_all.count())\n",
    "result = df.repartition('facility').rdd.mapPartitions(my_udf).toDF()\n",
    "result1 = df_all.repartition('facility','sim_id').rdd.mapPartitions(my_udf).toDF()\n",
    "\n",
    "df2 = df_all.select('facility','sim_id').groupBy('facility','sim_id').count()\n",
    "df2.count()\n",
    "#print(result.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()\n",
    "result1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SPARK-5063 Spark does not support nested RDDs or performing Spark actions inside of transformations\n",
    "PicklingError: Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference \n",
    "an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside\n",
    "of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values \n",
    "transformation and count action cannot be performed inside of the rdd1.map transformation. For more information.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    result = df.repartition('facility').rdd.mapPartitions(my_udf).toDF()\n",
    "    return result\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import random\n",
    "import time\n",
    "\n",
    "sc = spark.sparkContext\n",
    "#You Can't do this\n",
    "#df = sc.parallelize([time.time() + i for i in range(2)]) \\\n",
    "#            .map(func).toDF()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
