{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher-Order and Lambda Functions: Explore Complex and Structured Data in SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial walks you through four higher-order functions. While this in-depth [blog](https://databricks.com/blog/2017/05/24/working-with-nested-data-using-higher-order-functions-in-sql-on-databricks.html) explains the concepts, justifications, and motivations of _why_ handling complex data types such as arrays are important in SQL, and equally explains why their existing implementation are inefficient and cumbersome, this tutorial shows _how_ to use higher-order functions in SQL in processing structured data and arrays in IoT device events. In particular, they come handy and you can put them to good use if you enjoy functional programming and can quickly and can efficiently write a lambda expression as part of these higher-order SQL functions. \n",
    "\n",
    "This tutorial explores four functions and how you can put them to a wide range of uses in your processing and transforming array types:\n",
    "\n",
    "* `transform()`\n",
    "* `filter()`\n",
    "* `exists()`\n",
    "* `aggregate()`\n",
    "\n",
    "The takeaway from this short tutorial is that there exists myriad ways to slice and dice nested JSON structures with Spark SQL utility functions. These dedicated higher-order functions are primarily suited to manipulating arrays in Spark SQL, making it easier and the code more concise when processing table values with arrays or nested arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple JSON schema with attributes and values, with at least two attributes that are arrays, namely _temp_ and _c02_level_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"HigherOrderhu\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType() \\\n",
    "          .add(\"dc_id\", StringType()) \\\n",
    "          .add(\"source\", MapType(StringType(), StructType() \\\n",
    "                        .add(\"description\", StringType()) \\\n",
    "                        .add(\"ip\", StringType()) \\\n",
    "                        .add(\"id\", IntegerType()) \\\n",
    "                        .add(\"temp\", ArrayType(IntegerType())) \\\n",
    "                        .add(\"c02_level\", ArrayType(IntegerType())) \\\n",
    "                        .add(\"geo\", StructType() \\\n",
    "                              .add(\"lat\", DoubleType()) \\\n",
    "                              .add(\"long\", DoubleType()))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper Python function converts a JSON string into a [Python DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function for turning JSON strings into DataFrames.\n",
    "def jsonToDataFrame(json, schema=None):\n",
    "  # SparkSessions are available with Spark 2.0+\n",
    "  reader = spark.read\n",
    "  if schema:\n",
    "    reader.schema(schema)\n",
    "  return reader.json(sc.parallelize([json]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the schema above, create a complex JSON stucture and create into a Python DataFrame. Display the DataFrame gives us two columns: a key (dc_id) and value (source), which is JSON string with embedded nested structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "| dc_id|              source|\n",
      "+------+--------------------+\n",
      "|dc-101|[sensor-igauge ->...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF = jsonToDataFrame( \"\"\"{\n",
    "\n",
    "    \"dc_id\": \"dc-101\",\n",
    "    \"source\": {\n",
    "        \"sensor-igauge\": {\n",
    "        \"id\": 10,\n",
    "        \"ip\": \"68.28.91.22\",\n",
    "        \"description\": \"Sensor attached to the container ceilings\",\n",
    "        \"temp\":[35,35,35,36,35,35,32,35,30,35,32,35],\n",
    "        \"c02_level\": [1475,1476,1473],\n",
    "        \"geo\": {\"lat\":38.00, \"long\":97.00}                        \n",
    "      },\n",
    "      \"sensor-ipad\": {\n",
    "        \"id\": 13,\n",
    "        \"ip\": \"67.185.72.1\",\n",
    "        \"description\": \"Sensor ipad attached to carbon cylinders\",\n",
    "        \"temp\": [45,45,45,46,45,45,42,35,40,45,42,45],\n",
    "        \"c02_level\": [1370,1371,1372],\n",
    "        \"geo\": {\"lat\":47.41, \"long\":-122.00}\n",
    "      },\n",
    "      \"sensor-inest\": {\n",
    "        \"id\": 8,\n",
    "        \"ip\": \"208.109.163.218\",\n",
    "        \"description\": \"Sensor attached to the factory ceilings\",\n",
    "        \"temp\": [40,40,40,40,40,43,42,40,40,45,42,45],\n",
    "        \"c02_level\": [1346,1345, 1343],\n",
    "        \"geo\": {\"lat\":33.61, \"long\":-111.89}\n",
    "      },\n",
    "      \"sensor-istick\": {\n",
    "        \"id\": 5,\n",
    "        \"ip\": \"204.116.105.67\",\n",
    "        \"description\": \"Sensor embedded in exhaust pipes in the ceilings\",\n",
    "        \"temp\":[30,30,30,30,40,43,42,40,40,35,42,35],\n",
    "        \"c02_level\": [1574,1570, 1576],\n",
    "        \"geo\": {\"lat\":35.93, \"long\":-85.46}\n",
    "      }\n",
    "    }\n",
    "  }\"\"\", schema)\n",
    "\n",
    "dataDF.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining its schema, you can notice that the DataFrame schema reflects the above defined schema, where two of its elments are are arrays of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dc_id: string (nullable = true)\n",
      " |-- source: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- ip: string (nullable = true)\n",
      " |    |    |-- id: integer (nullable = true)\n",
      " |    |    |-- temp: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |-- c02_level: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |-- geo: struct (nullable = true)\n",
      " |    |    |    |-- lat: double (nullable = true)\n",
      " |    |    |    |-- long: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employ `explode()` to explode the column `source` into its individual columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+\n",
      "| dc_id|          key|               value|\n",
      "+------+-------------+--------------------+\n",
      "|dc-101|sensor-igauge|[Sensor attached ...|\n",
      "|dc-101|  sensor-ipad|[Sensor ipad atta...|\n",
      "|dc-101| sensor-inest|[Sensor attached ...|\n",
      "|dc-101|sensor-istick|[Sensor embedded ...|\n",
      "+------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explodedDF = dataDF.select(\"dc_id\", explode(\"source\"))\n",
    "explodedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can work with the `value` column, which a is `struct`, to extract individual fields by using their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------------+---------+------------------+--------------------+\n",
      "| dc_id|          key|             ip|device_id|        c02_levels|                temp|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+\n",
      "|dc-101|sensor-igauge|    68.28.91.22|       10|[1475, 1476, 1473]|[35, 35, 35, 36, ...|\n",
      "|dc-101|  sensor-ipad|    67.185.72.1|       13|[1370, 1371, 1372]|[45, 45, 45, 46, ...|\n",
      "|dc-101| sensor-inest|208.109.163.218|        8|[1346, 1345, 1343]|[40, 40, 40, 40, ...|\n",
      "|dc-101|sensor-istick| 204.116.105.67|        5|[1574, 1570, 1576]|[30, 30, 30, 30, ...|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# use col.getItem(key) to get individual values within our Map\n",
    "#\n",
    "devicesDataDF = explodedDF.select(\"dc_id\", \"key\", \\\n",
    "                        \"value.ip\", \\\n",
    "                        col(\"value.id\").alias(\"device_id\"), \\\n",
    "                        col(\"value.c02_level\").alias(\"c02_levels\"), \\\n",
    "                        \"value.temp\")\n",
    "devicesDataDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sanity let's ensure what was created as DataFrame was preserved and adherent to the schema declared above while exploding and extracting individual data items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dc_id: string (nullable = true)\n",
      " |-- key: string (nullable = false)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- c02_levels: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- temp: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "devicesDataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since this tutorial is less about DataFrames API and more about higher-order functions and lambdas in SQL, create a temporary table or view and start using the higher-order SQL functions mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "devicesDataDF.createOrReplaceTempView(\"data_center_iot_devices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table was created as columns in your DataFrames and it reflects its schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------------+---------+------------------+--------------------+\n",
      "| dc_id|          key|             ip|device_id|        c02_levels|                temp|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+\n",
      "|dc-101|sensor-igauge|    68.28.91.22|       10|[1475, 1476, 1473]|[35, 35, 35, 36, ...|\n",
      "|dc-101|  sensor-ipad|    67.185.72.1|       13|[1370, 1371, 1372]|[45, 45, 45, 46, ...|\n",
      "|dc-101| sensor-inest|208.109.163.218|        8|[1346, 1345, 1343]|[40, 40, 40, 40, ...|\n",
      "|dc-101|sensor-istick| 204.116.105.67|        5|[1574, 1570, 1576]|[30, 30, 30, 30, ...|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from data_center_iot_devices\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|  col_name| data_type|comment|\n",
      "+----------+----------+-------+\n",
      "|     dc_id|    string|   null|\n",
      "|       key|    string|   null|\n",
      "|        ip|    string|   null|\n",
      "| device_id|       int|   null|\n",
      "|c02_levels|array<int>|   null|\n",
      "|      temp|array<int>|   null|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe data_center_iot_devices\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Higher-Order Functions and Lambda Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use `transform()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its functional signature, `transform(values, value -> lambda expression)`, has two components:\n",
    "\n",
    "1. `transform(values..)` is the higher-order function. This takes an array and an anonymous function as its input. Internally, `transform` takes care of setting up a new array, applying the anonymous function to each element, and then assigning the result to the output array.\n",
    "2.  The `value -> expression`  is an anonymous function. The function is further divided into two components separated by a `->` symbol:\n",
    "  * **The argument list**: This case has only one argument: value. You can specify multiple arguments by creating a comma-separated list of arguments enclosed by parenthesis, for example: `(x, y) -> x + y.`\n",
    "  * **The body**: This is a SQL expression that can use the arguments and outer variables to calculate the new value. \n",
    "  \n",
    "In short, the programmatic signature for `transform()` is as follows:\n",
    "\n",
    "`transform(array<T>, function<T, U>): array<U>`\n",
    "This produces an array by applying a function<T, U> to each element of an input array<T>.\n",
    "Note that the functional programming equivalent operation is `map`. This has been named transform in order to prevent confusion with the map expression (that creates a map from a key value expression).\n",
    "\n",
    "This basic scheme for `transform(...)` works the same way as with other higher-order functions, as you will see shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following query transforms the values in an array by converting each elmement's temperature reading from Celsius to Fahrenheit.\n",
    "\n",
    "Let's transform (and hence convert) all our Celsius reading into Fahrenheit. (Use conversion formula: `((C * 9) / 5) + 32`) The lambda expression here is the formula to convert **C->F**.\n",
    "Now, `temp` and `((t * 9) div 5) + 32` are the arguments to the higher-order function `transform()`. The anonymous function will iterate through each element in the array, `temp`, apply the function to it, and transforming its value and placing into an output array. The result is a new column with tranformed values: `fahrenheit_temp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------+--------------------+--------------------+\n",
      "|          key|             ip|device_id|                temp|     fahrenheit_temp|\n",
      "+-------------+---------------+---------+--------------------+--------------------+\n",
      "|sensor-igauge|    68.28.91.22|       10|[35, 35, 35, 36, ...|[95, 95, 95, 96, ...|\n",
      "|  sensor-ipad|    67.185.72.1|       13|[45, 45, 45, 46, ...|[113, 113, 113, 1...|\n",
      "| sensor-inest|208.109.163.218|        8|[40, 40, 40, 40, ...|[104, 104, 104, 1...|\n",
      "|sensor-istick| 204.116.105.67|        5|[30, 30, 30, 30, ...|[86, 86, 86, 86, ...|\n",
      "+-------------+---------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select key, ip, device_id, temp,\n",
    "     transform (temp, t -> ((t * 9) div 5) + 32 ) as fahrenheit_temp\n",
    "     from data_center_iot_devices\n",
    "     \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above example generates transformed values, this example uses a Boolean expression as a lambda function and generates a boolean array of results instead of values, since the expression \n",
    "`t->t > 1300` is a predicate, resulting into a true or false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------------+---------+------------------+--------------------+--------------------+\n",
      "| dc_id|          key|             ip|device_id|        c02_levels|                temp|     high_c02_levels|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+--------------------+\n",
      "|dc-101|sensor-igauge|    68.28.91.22|       10|[1475, 1476, 1473]|[35, 35, 35, 36, ...|  [true, true, true]|\n",
      "|dc-101|  sensor-ipad|    67.185.72.1|       13|[1370, 1371, 1372]|[45, 45, 45, 46, ...|[false, false, fa...|\n",
      "|dc-101| sensor-inest|208.109.163.218|        8|[1346, 1345, 1343]|[40, 40, 40, 40, ...|[false, false, fa...|\n",
      "|dc-101|sensor-istick| 204.116.105.67|        5|[1574, 1570, 1576]|[30, 30, 30, 30, ...|  [true, true, true]|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select dc_id, key, ip, device_id, c02_levels, temp, \n",
    "                 transform (c02_levels, t -> t > 1400) as high_c02_levels\n",
    "                 from data_center_iot_devices\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use `filter()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with `transform`, `filter` has a similar signature, `filter(array<T>, function<T, Boolean>): array<T>`\n",
    "Unlike `transform()` with a boolean expression, it produces an output array from an input array by *only* adding elements for which predicate `function<T, Boolean>` holds.\n",
    "\n",
    "For instance, let's include only readings in our `c02_levels` that exceed dangerous levels (`cO2_level > 1300`). Again the functional signature is not dissimilar to `transform()`. However, note the difference in how `filter()` generated the resulting array compared to _transform()_ with similar lambda expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------------+---------+------------------+--------------------+------------------+\n",
      "| dc_id|          key|             ip|device_id|        c02_levels|                temp|   high_c02_levels|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+------------------+\n",
      "|dc-101|sensor-igauge|    68.28.91.22|       10|[1475, 1476, 1473]|[35, 35, 35, 36, ...|[1475, 1476, 1473]|\n",
      "|dc-101|  sensor-ipad|    67.185.72.1|       13|[1370, 1371, 1372]|[45, 45, 45, 46, ...|[1370, 1371, 1372]|\n",
      "|dc-101| sensor-inest|208.109.163.218|        8|[1346, 1345, 1343]|[40, 40, 40, 40, ...|[1346, 1345, 1343]|\n",
      "|dc-101|sensor-istick| 204.116.105.67|        5|[1574, 1570, 1576]|[30, 30, 30, 30, ...|[1574, 1570, 1576]|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select dc_id, key, ip, device_id, c02_levels, temp, \n",
    "     filter (c02_levels, t -> t > 1300) as high_c02_levels\n",
    "     from data_center_iot_devices\n",
    "     \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice when the lambda's predicate expression is reversed, the resulting array is empty. That is, it does not evaluate to values true or false as it did in `tranform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------------+---------+------------------+--------------------+---------------+\n",
      "| dc_id|          key|             ip|device_id|        c02_levels|                temp|high_c02_levels|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+---------------+\n",
      "|dc-101|sensor-igauge|    68.28.91.22|       10|[1475, 1476, 1473]|[35, 35, 35, 36, ...|             []|\n",
      "|dc-101|  sensor-ipad|    67.185.72.1|       13|[1370, 1371, 1372]|[45, 45, 45, 46, ...|             []|\n",
      "|dc-101| sensor-inest|208.109.163.218|        8|[1346, 1345, 1343]|[40, 40, 40, 40, ...|             []|\n",
      "|dc-101|sensor-istick| 204.116.105.67|        5|[1574, 1570, 1576]|[30, 30, 30, 30, ...|             []|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select dc_id, key, ip, device_id, c02_levels, temp, \n",
    "     filter (c02_levels, t -> t < 1300 ) as high_c02_levels\n",
    "     from data_center_iot_devices\n",
    "     \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use `exists()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mildly different functional signature than the above two functions, the idea is simple and same: \n",
    "\n",
    "`exists(array<T>, function<T, V, Boolean>): Boolean`\n",
    "Return true if predicate `function<T, Boolean>` holds for any element in input array.\n",
    "\n",
    "In this case you can iterate through the `temp` array and see if a particular value exists in the array. Let's acertain if any of your values contains 45 degrees Celsius or determine of a c02 level in any of the readings equals to 1570."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------------+---------+------------------+--------------------+------------+\n",
      "| dc_id|          key|             ip|device_id|        c02_levels|                temp|value_exists|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+------------+\n",
      "|dc-101|sensor-igauge|    68.28.91.22|       10|[1475, 1476, 1473]|[35, 35, 35, 36, ...|       false|\n",
      "|dc-101|  sensor-ipad|    67.185.72.1|       13|[1370, 1371, 1372]|[45, 45, 45, 46, ...|        true|\n",
      "|dc-101| sensor-inest|208.109.163.218|        8|[1346, 1345, 1343]|[40, 40, 40, 40, ...|        true|\n",
      "|dc-101|sensor-istick| 204.116.105.67|        5|[1574, 1570, 1576]|[30, 30, 30, 30, ...|       false|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select dc_id, key, ip, device_id, c02_levels, temp, \n",
    "     exists (temp, t -> t = 45 ) as value_exists\n",
    "     from data_center_iot_devices\n",
    "     \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------------+---------+------------------+--------------------+---------------+\n",
      "| dc_id|          key|             ip|device_id|        c02_levels|                temp|high_c02_levels|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+---------------+\n",
      "|dc-101|sensor-igauge|    68.28.91.22|       10|[1475, 1476, 1473]|[35, 35, 35, 36, ...|          false|\n",
      "|dc-101|  sensor-ipad|    67.185.72.1|       13|[1370, 1371, 1372]|[45, 45, 45, 46, ...|          false|\n",
      "|dc-101| sensor-inest|208.109.163.218|        8|[1346, 1345, 1343]|[40, 40, 40, 40, ...|          false|\n",
      "|dc-101|sensor-istick| 204.116.105.67|        5|[1574, 1570, 1576]|[30, 30, 30, 30, ...|           true|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select dc_id, key, ip, device_id, c02_levels, temp, \n",
    "     exists (c02_levels, t -> t = 1570 ) as high_c02_levels\n",
    "     from data_center_iot_devices\n",
    "     \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use `reduce()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By far this function and its method is more advanced than others. It also allows you to do aggregation, as seen in the next section.\n",
    "Its signature allows us to some extra bit with the last lambda expression as its functional argument.\n",
    "\n",
    "`reduce(array<T>, B, function<B, T, B>, function<B, R>): R`\n",
    "Reduce the elements of `array<T>` into a single value `R` by merging the elements into a buffer B using `function<B, T, B>` and by applying a finish `function<B, R>` on the final buffer. The initial value `B` is determined by a `zero` expression. \n",
    "\n",
    "The finalize function is optional, if you do not specify the function the finalize function the identity function (`id -> id`) is used.\n",
    "This is the only higher-order function that takes two lambda functions.\n",
    "\n",
    "For instance, if you want to compute an average of the temperature readings, use lambda expressions: The first one accumulates all the results into an internal temporary buffer, and the second function applies to the final accumulated buffer. With respect to our signature above, `B` is `0`; `function<B,T,B>` is `t + acc`, and `function<B,R>` is `acc div size(temp)`. Furthermore, in the finalize lambda expression, convert the average temperature to Fahrenheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 31))\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"Undefined function: 'reduce'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 2 pos 4\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: Undefined function: 'reduce'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 2 pos 4\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15$$anonfun$applyOrElse$49.apply(Analyzer.scala:1280)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15$$anonfun$applyOrElse$49.apply(Analyzer.scala:1280)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1279)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1271)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:256)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:256)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:261)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:83)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:83)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:83)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:74)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressions$1.applyOrElse(AnalysisHelper.scala:129)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressions$1.applyOrElse(AnalysisHelper.scala:128)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$apply$6.apply(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$apply$6.apply(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveExpressions(AnalysisHelper.scala:128)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressions(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:1271)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:1268)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-be21e08ef95d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m->\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m \u001b[0mdiv\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m9\u001b[0m \u001b[0mdiv\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0maverage_f_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdata_center_iot_devices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     sort by average_f_temp desc\"\"\").show()\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Undefined function: 'reduce'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 2 pos 4\""
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select key, ip, device_id, temp,\n",
    "    reduce(temp, 0, (t, acc) -> t + acc, acc-> (acc div size(temp) * 9 div 5) + 32 ) as average_f_temp\n",
    "    from data_center_iot_devices\n",
    "    sort by average_f_temp desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simillarly, `reduce()` is employed here to get an average of c02_levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" select key, ip, device_id, c02_levels,\n",
    "    reduce(c02_levels, 0, (t, acc) -> t + acc, acc-> acc div size(c02_levels)) as average_c02_levels\n",
    "    from data_center_iot_devices\n",
    "    sort by  average_c02_levels desc\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use `aggregate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate is an alias of `reduce`. It has the same inputs, and it will produce the same results.\n",
    "  \n",
    "Let's compute a geomean of the c02 levels and sort them by descending order. Note the complex lambda expression with the above functional signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 5))\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve 'aggregate(data_center_iot_devices.`c02_levels`, named_struct('product', 1.0BD, 'N', 0), lambdafunction(named_struct('col1', (CAST(CAST(namedlambdavariable() AS DECIMAL(10,0)) AS DECIMAL(11,1)) * CAST(namedlambdavariable().`product` AS DECIMAL(11,1))), 'col2', (namedlambdavariable().`N` + 1)), namedlambdavariable(), namedlambdavariable()), lambdafunction(round(POWER(CAST(namedlambdavariable().`product` AS DOUBLE), CAST((CAST(1.0BD AS DECIMAL(11,1)) / CAST(CAST(namedlambdavariable().`N` AS DECIMAL(10,0)) AS DECIMAL(11,1))) AS DOUBLE)), 0), namedlambdavariable()))' due to data type mismatch: argument 3 requires struct<product:decimal(2,1),N:int> type, however, 'lambdafunction(named_struct('col1', (CAST(CAST(namedlambdavariable() AS DECIMAL(10,0)) AS DECIMAL(11,1)) * CAST(namedlambdavariable().`product` AS DECIMAL(11,1))), 'col2', (namedlambdavariable().`N` + 1)), namedlambdavariable(), namedlambdavariable())' is of struct<col1:decimal(13,1),col2:int> type.; line 2 pos 5;\\nSort [c02_geomean#284 DESC NULLS LAST], false\\n+- Project [key#16, ip#33, device_id#31, c02_levels#32, aggregate(c02_levels#32, named_struct(product, 1.0, N, 0), lambdafunction(named_struct(col1, CheckOverflow((promote_precision(cast(cast(lambda c02#286 as decimal(10,0)) as decimal(11,1))) * promote_precision(cast(lambda buffer#285.product as decimal(11,1)))), DecimalType(13,1)), col2, (lambda buffer#285.N + 1)), lambda buffer#285, lambda c02#286, false), lambdafunction(round(POWER(cast(lambda buffer#287.product as double), cast(CheckOverflow((promote_precision(cast(1.0 as decimal(11,1))) / promote_precision(cast(cast(lambda buffer#287.N as decimal(10,0)) as decimal(11,1)))), DecimalType(13,12)) as double)), 0), lambda buffer#287, false)) AS c02_geomean#284]\\n   +- SubqueryAlias `data_center_iot_devices`\\n      +- Project [dc_id#5, key#16, value#17.ip AS ip#33, value#17.id AS device_id#31, value#17.c02_level AS c02_levels#32, value#17.temp AS temp#36]\\n         +- Project [dc_id#5, key#16, value#17]\\n            +- Generate explode(source#6), false, [key#16, value#17]\\n               +- LogicalRDD [dc_id#5, source#6], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve 'aggregate(data_center_iot_devices.`c02_levels`, named_struct('product', 1.0BD, 'N', 0), lambdafunction(named_struct('col1', (CAST(CAST(namedlambdavariable() AS DECIMAL(10,0)) AS DECIMAL(11,1)) * CAST(namedlambdavariable().`product` AS DECIMAL(11,1))), 'col2', (namedlambdavariable().`N` + 1)), namedlambdavariable(), namedlambdavariable()), lambdafunction(round(POWER(CAST(namedlambdavariable().`product` AS DOUBLE), CAST((CAST(1.0BD AS DECIMAL(11,1)) / CAST(CAST(namedlambdavariable().`N` AS DECIMAL(10,0)) AS DECIMAL(11,1))) AS DOUBLE)), 0), namedlambdavariable()))' due to data type mismatch: argument 3 requires struct<product:decimal(2,1),N:int> type, however, 'lambdafunction(named_struct('col1', (CAST(CAST(namedlambdavariable() AS DECIMAL(10,0)) AS DECIMAL(11,1)) * CAST(namedlambdavariable().`product` AS DECIMAL(11,1))), 'col2', (namedlambdavariable().`N` + 1)), namedlambdavariable(), namedlambdavariable())' is of struct<col1:decimal(13,1),col2:int> type.; line 2 pos 5;\nSort [c02_geomean#284 DESC NULLS LAST], false\n+- Project [key#16, ip#33, device_id#31, c02_levels#32, aggregate(c02_levels#32, named_struct(product, 1.0, N, 0), lambdafunction(named_struct(col1, CheckOverflow((promote_precision(cast(cast(lambda c02#286 as decimal(10,0)) as decimal(11,1))) * promote_precision(cast(lambda buffer#285.product as decimal(11,1)))), DecimalType(13,1)), col2, (lambda buffer#285.N + 1)), lambda buffer#285, lambda c02#286, false), lambdafunction(round(POWER(cast(lambda buffer#287.product as double), cast(CheckOverflow((promote_precision(cast(1.0 as decimal(11,1))) / promote_precision(cast(cast(lambda buffer#287.N as decimal(10,0)) as decimal(11,1)))), DecimalType(13,12)) as double)), 0), lambda buffer#287, false)) AS c02_geomean#284]\n   +- SubqueryAlias `data_center_iot_devices`\n      +- Project [dc_id#5, key#16, value#17.ip AS ip#33, value#17.id AS device_id#31, value#17.c02_level AS c02_levels#32, value#17.temp AS temp#36]\n         +- Project [dc_id#5, key#16, value#17]\n            +- Generate explode(source#6), false, [key#16, value#17]\n               +- LogicalRDD [dc_id#5, source#6], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4b83ec6bb7b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m      \u001b[0;32mfrom\u001b[0m \u001b[0mdata_center_iot_devices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m      \u001b[0msort\u001b[0m \u001b[0mby\u001b[0m \u001b[0mc02_geomean\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m      \"\"\").show()\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve 'aggregate(data_center_iot_devices.`c02_levels`, named_struct('product', 1.0BD, 'N', 0), lambdafunction(named_struct('col1', (CAST(CAST(namedlambdavariable() AS DECIMAL(10,0)) AS DECIMAL(11,1)) * CAST(namedlambdavariable().`product` AS DECIMAL(11,1))), 'col2', (namedlambdavariable().`N` + 1)), namedlambdavariable(), namedlambdavariable()), lambdafunction(round(POWER(CAST(namedlambdavariable().`product` AS DOUBLE), CAST((CAST(1.0BD AS DECIMAL(11,1)) / CAST(CAST(namedlambdavariable().`N` AS DECIMAL(10,0)) AS DECIMAL(11,1))) AS DOUBLE)), 0), namedlambdavariable()))' due to data type mismatch: argument 3 requires struct<product:decimal(2,1),N:int> type, however, 'lambdafunction(named_struct('col1', (CAST(CAST(namedlambdavariable() AS DECIMAL(10,0)) AS DECIMAL(11,1)) * CAST(namedlambdavariable().`product` AS DECIMAL(11,1))), 'col2', (namedlambdavariable().`N` + 1)), namedlambdavariable(), namedlambdavariable())' is of struct<col1:decimal(13,1),col2:int> type.; line 2 pos 5;\\nSort [c02_geomean#284 DESC NULLS LAST], false\\n+- Project [key#16, ip#33, device_id#31, c02_levels#32, aggregate(c02_levels#32, named_struct(product, 1.0, N, 0), lambdafunction(named_struct(col1, CheckOverflow((promote_precision(cast(cast(lambda c02#286 as decimal(10,0)) as decimal(11,1))) * promote_precision(cast(lambda buffer#285.product as decimal(11,1)))), DecimalType(13,1)), col2, (lambda buffer#285.N + 1)), lambda buffer#285, lambda c02#286, false), lambdafunction(round(POWER(cast(lambda buffer#287.product as double), cast(CheckOverflow((promote_precision(cast(1.0 as decimal(11,1))) / promote_precision(cast(cast(lambda buffer#287.N as decimal(10,0)) as decimal(11,1)))), DecimalType(13,12)) as double)), 0), lambda buffer#287, false)) AS c02_geomean#284]\\n   +- SubqueryAlias `data_center_iot_devices`\\n      +- Project [dc_id#5, key#16, value#17.ip AS ip#33, value#17.id AS device_id#31, value#17.c02_level AS c02_levels#32, value#17.temp AS temp#36]\\n         +- Project [dc_id#5, key#16, value#17]\\n            +- Generate explode(source#6), false, [key#16, value#17]\\n               +- LogicalRDD [dc_id#5, source#6], false\\n\""
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select key, ip, device_id, c02_levels,\n",
    "     aggregate(c02_levels,\n",
    "               (1.0 as product, 0 as N),\n",
    "               (buffer, c02) -> (c02 * buffer.product, buffer.N+1),\n",
    "               buffer -> round(Power(buffer.product, 1.0 / buffer.N))) as c02_geomean\n",
    "     from data_center_iot_devices\n",
    "     sort by c02_geomean desc\n",
    "     \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example using similar nested structure with IoT JSON data.\n",
    "\n",
    "Let's create a DataFrame based on this schema and check if all is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2 = StructType() \\\n",
    "                    .add(\"device_id\", IntegerType()) \\\n",
    "                    .add(\"battery_level\", ArrayType(IntegerType())) \\\n",
    "                    .add(\"c02_level\", ArrayType(IntegerType())) \\\n",
    "                    .add(\"signal\", ArrayType(IntegerType())) \\\n",
    "                    .add(\"temp\", ArrayType(IntegerType())) \\\n",
    "                    .add(\"cca3\", ArrayType(StringType())) \\\n",
    "                    .add(\"device_type\", StringType()) \\\n",
    "                    .add(\"ip\", StringType()) \\\n",
    "                    .add(\"timestamp\", TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+------------------+------------+------------+--------------------+-------------+---------------+-------------------+\n",
      "|device_id|battery_level|         c02_level|      signal|        temp|                cca3|  device_type|             ip|          timestamp|\n",
      "+---------+-------------+------------------+------------+------------+--------------------+-------------+---------------+-------------------+\n",
      "|        0|    [8, 9, 7]|   [917, 921, 925]|[23, 22, 24]|[25, 26, 27]|[USA, United States]|  sensor-ipad|   68.161.225.1|2016-10-04 22:31:36|\n",
      "|        1|    [6, 6, 5]|[1413, 1416, 1417]|[18, 18, 19]|[30, 32, 35]|       [NOR, Norway]|sensor-igauge|  213.161.254.1|2016-10-04 22:31:38|\n",
      "|        3|    [1, 1, 0]|[1447, 1446, 1448]|[12, 12, 13]|[47, 47, 48]|[USA, United States]| sensor-inest|  66.39.173.154|2016-10-04 22:31:42|\n",
      "|        4|    [0, 0, 0]|   [983, 990, 982]|[11, 11, 11]|[29, 29, 28]|  [PHL, Philippines]|  sensor-ipad|    203.82.41.9|2016-10-04 22:31:44|\n",
      "|        5|    [8, 8, 8]|[1574, 1575, 1576]|[16, 16, 17]|[50, 51, 50]|[USA, United States]|sensor-istick| 204.116.105.67|2016-10-04 22:31:46|\n",
      "|        6|    [9, 9, 9]|[1249, 1249, 1250]|[18, 18, 19]|[21, 21, 22]|        [CHN, China]|  sensor-ipad|  220.173.179.1|2016-10-04 22:31:48|\n",
      "|        7|    [0, 0, 0]|[1531, 1532, 1531]|[15, 15, 29]|[27, 27, 28]|        [JPN, Japan]|  sensor-ipad|  118.23.68.227|2016-10-04 22:31:52|\n",
      "|        8|   [9, 9, 10]|[1208, 1209, 1208]|[16, 16, 17]|[40, 40, 41]|[USA, United States]| sensor-inest|208.109.163.218|2016-10-04 22:31:54|\n",
      "|        9|   [0, -1, 0]|[1171, 1240, 1400]| [11, 5, 24]| [19, 28, 5]|        [ITA, Italy]|  sensor-ipad|  88.213.191.34|2016-10-04 22:31:56|\n",
      "|       10|    [7, 7, 8]|   [886, 886, 887]|[26, 26, 25]|[32, 33, 32]|[USA, United States]|sensor-igauge|    68.28.91.22|2016-10-04 22:31:58|\n",
      "|       11|    [4, 5, 5]|   [863, 862, 864]|[25, 25, 24]|[46, 45, 44]|        [IND, India]|  sensor-ipad| 59.144.114.250|2016-10-04 22:32:00|\n",
      "|       12|    [8, 9, 8]|[1220, 1221, 1220]|[26, 25, 26]|[18, 17, 18]|       [NOR, Norway]|sensor-igauge| 193.156.90.200|2016-10-04 22:32:02|\n",
      "|       13|    [8, 8, 8]|[1504, 1504, 1503]|[20, 21, 20]|[34, 35, 34]|[USA, United States]|  sensor-ipad|    67.185.72.1|2016-10-04 22:32:04|\n",
      "|       14|    [8, 8, 7]|   [831, 832, 831]|[17, 17, 18]|[39, 40, 38]|[USA, United States]| sensor-inest|   68.85.85.106|2016-10-04 22:32:06|\n",
      "|       15|    [5, 5, 5]|[1378, 1376, 1378]|[26, 26, 25]|[27, 27, 28]|[USA, United States]|  sensor-ipad|161.188.212.254|2016-10-04 22:32:08|\n",
      "|       16|    [6, 5, 6]|[1423, 1423, 1423]|[24, 24, 23]|[10, 10, 11]|        [CHN, China]|sensor-igauge|  221.3.128.242|2016-10-04 22:32:10|\n",
      "|       17|    [9, 9, 9]|[1304, 1304, 1304]|[17, 17, 17]|[38, 38, 39]|[USA, United States]|  sensor-ipad| 64.124.180.215|2016-10-04 22:32:12|\n",
      "|       18|    [0, 0, 0]|  [902, 902, 1300]|  [10, 1, 5]| [26, 0, 99]|[USA, United States]|sensor-igauge|  66.153.162.66|2016-10-04 22:32:14|\n",
      "|       19|    [5, 5, 5]|[1282, 1282, 1281]|[27, 27, 28]|[32, 32, 33]|      [AUT, Austria]|  sensor-ipad|193.200.142.254|2016-10-04 22:32:16|\n",
      "+---------+-------------+------------------+------------+------------+--------------------+-------------+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF2 = jsonToDataFrame(\"\"\"[\n",
    "  {\"device_id\": 0, \"device_type\": \"sensor-ipad\", \"ip\": \"68.161.225.1\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [25,26, 27], \"signal\": [23,22,24], \"battery_level\": [8,9,7], \"c02_level\": [917, 921, 925], \"timestamp\" :1475600496 }, \n",
    "  {\"device_id\": 1, \"device_type\": \"sensor-igauge\", \"ip\": \"213.161.254.1\", \"cca3\": [\"NOR\", \"Norway\"], \"temp\": [30, 32,35], \"signal\": [18,18,19], \"battery_level\": [6, 6, 5], \"c02_level\": [1413, 1416, 1417], \"timestamp\" :1475600498 }, \n",
    "  {\"device_id\": 3, \"device_type\": \"sensor-inest\", \"ip\": \"66.39.173.154\", \"cca3\": [\"USA\", \"United States\"], \"temp\":[47, 47, 48], \"signal\": [12,12,13], \"battery_level\": [1, 1, 0],  \"c02_level\": [1447,1446, 1448], \"timestamp\" :1475600502 }, \n",
    "  {\"device_id\": 4, \"device_type\": \"sensor-ipad\", \"ip\": \"203.82.41.9\", \"cca3\":[\"PHL\", \"Philippines\"], \"temp\":[29, 29, 28], \"signal\":[11, 11, 11], \"battery_level\":[0, 0, 0], \"c02_level\": [983, 990, 982], \"timestamp\" :1475600504 },\n",
    "  {\"device_id\": 5, \"device_type\": \"sensor-istick\", \"ip\": \"204.116.105.67\", \"cca3\": [\"USA\", \"United States\"], \"temp\":[50,51,50], \"signal\": [16,16,17], \"battery_level\": [8,8, 8], \"c02_level\": [1574,1575,1576], \"timestamp\" :1475600506 }, \n",
    "  {\"device_id\": 6, \"device_type\": \"sensor-ipad\", \"ip\": \"220.173.179.1\", \"cca3\": [\"CHN\", \"China\"], \"temp\": [21,21,22], \"signal\": [18,18,19], \"battery_level\": [9,9,9], \"c02_level\": [1249,1249,1250], \"timestamp\" :1475600508 },\n",
    "  {\"device_id\": 7, \"device_type\": \"sensor-ipad\", \"ip\": \"118.23.68.227\", \"cca3\": [\"JPN\", \"Japan\"], \"temp\":[27,27,28], \"signal\": [15,15,29], \"battery_level\":[0,0,0], \"c02_level\": [1531,1532,1531], \"timestamp\" :1475600512 },\n",
    "  {\"device_id\": 8, \"device_type\": \"sensor-inest\", \"ip\": \"208.109.163.218\", \"cca3\": [\"USA\", \"United States\"], \"temp\":[40,40,41], \"signal\": [16,16,17], \"battery_level\":[ 9, 9, 10], \"c02_level\": [1208,1209,1208], \"timestamp\" :1475600514},\n",
    "  {\"device_id\": 9, \"device_type\": \"sensor-ipad\", \"ip\": \"88.213.191.34\", \"cca3\": [\"ITA\", \"Italy\"], \"temp\": [19,28,5], \"signal\": [11, 5, 24], \"battery_level\": [0,-1,0], \"c02_level\": [1171, 1240, 1400], \"timestamp\" :1475600516 },\n",
    "  {\"device_id\": 10, \"device_type\": \"sensor-igauge\", \"ip\": \"68.28.91.22\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [32,33,32], \"signal\": [26,26,25], \"battery_level\": [7,7,8], \"c02_level\": [886,886,887], \"timestamp\" :1475600518 },\n",
    "  {\"device_id\": 11, \"device_type\": \"sensor-ipad\", \"ip\": \"59.144.114.250\", \"cca3\": [\"IND\", \"India\"], \"temp\": [46,45,44], \"signal\": [25,25,24], \"battery_level\": [4,5,5], \"c02_level\": [863,862,864], \"timestamp\" :1475600520 },\n",
    "  {\"device_id\": 12, \"device_type\": \"sensor-igauge\", \"ip\": \"193.156.90.200\", \"cca3\": [\"NOR\", \"Norway\"], \"temp\": [18,17,18], \"signal\": [26,25,26], \"battery_level\": [8,9,8], \"c02_level\": [1220,1221,1220], \"timestamp\" :1475600522 },\n",
    "  {\"device_id\": 13, \"device_type\": \"sensor-ipad\", \"ip\": \"67.185.72.1\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [34,35,34], \"signal\": [20,21,20], \"battery_level\": [8,8,8], \"c02_level\": [1504,1504,1503], \"timestamp\" :1475600524 },\n",
    "  {\"device_id\": 14, \"device_type\": \"sensor-inest\", \"ip\": \"68.85.85.106\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [39,40,38], \"signal\": [17, 17, 18], \"battery_level\": [8,8,7], \"c02_level\": [831,832,831], \"timestamp\" :1475600526 },\n",
    "  {\"device_id\": 15, \"device_type\": \"sensor-ipad\", \"ip\": \"161.188.212.254\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [27,27,28], \"signal\": [26,26,25], \"battery_level\": [5,5,5], \"c02_level\": [1378,1376,1378], \"timestamp\" :1475600528 },\n",
    "  {\"device_id\": 16, \"device_type\": \"sensor-igauge\", \"ip\": \"221.3.128.242\", \"cca3\": [\"CHN\", \"China\"], \"temp\": [10,10,11], \"signal\": [24,24,23], \"battery_level\": [6,5,6], \"c02_level\": [1423, 1423, 1423], \"timestamp\" :1475600530 },\n",
    "  {\"device_id\": 17, \"device_type\": \"sensor-ipad\", \"ip\": \"64.124.180.215\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [38,38,39], \"signal\": [17,17,17], \"battery_level\": [9,9,9], \"c02_level\": [1304,1304,1304], \"timestamp\" :1475600532 },\n",
    "  {\"device_id\": 18, \"device_type\": \"sensor-igauge\", \"ip\": \"66.153.162.66\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [26, 0, 99], \"signal\": [10, 1, 5], \"battery_level\": [0, 0, 0], \"c02_level\": [902,902, 1300], \"timestamp\" :1475600534 },\n",
    "  {\"device_id\": 19, \"device_type\": \"sensor-ipad\", \"ip\": \"193.200.142.254\", \"cca3\": [\"AUT\", \"Austria\"], \"temp\": [32,32,33], \"signal\": [27,27,28], \"battery_level\": [5,5,5], \"c02_level\": [1282, 1282, 1281], \"timestamp\" :1475600536 }\n",
    "  ]\"\"\", schema2)\n",
    "\n",
    "dataDF2.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- battery_level: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- c02_level: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- signal: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- temp: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- cca3: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, let's create a temporary view to which you can issue SQL queries and do some processing using higher-order functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF2.createOrReplaceTempView(\"iot_nested_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use `transform()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use transform to check battery level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------------+---------------------+\n",
      "|                cca3|  device_type|battery_level|boolean_battery_level|\n",
      "+--------------------+-------------+-------------+---------------------+\n",
      "|[USA, United States]|  sensor-ipad|    [8, 9, 7]|   [true, true, true]|\n",
      "|       [NOR, Norway]|sensor-igauge|    [6, 6, 5]|   [true, true, true]|\n",
      "|[USA, United States]| sensor-inest|    [1, 1, 0]|  [true, true, false]|\n",
      "|  [PHL, Philippines]|  sensor-ipad|    [0, 0, 0]| [false, false, fa...|\n",
      "|[USA, United States]|sensor-istick|    [8, 8, 8]|   [true, true, true]|\n",
      "|        [CHN, China]|  sensor-ipad|    [9, 9, 9]|   [true, true, true]|\n",
      "|        [JPN, Japan]|  sensor-ipad|    [0, 0, 0]| [false, false, fa...|\n",
      "|[USA, United States]| sensor-inest|   [9, 9, 10]|   [true, true, true]|\n",
      "|        [ITA, Italy]|  sensor-ipad|   [0, -1, 0]| [false, false, fa...|\n",
      "|[USA, United States]|sensor-igauge|    [7, 7, 8]|   [true, true, true]|\n",
      "|        [IND, India]|  sensor-ipad|    [4, 5, 5]|   [true, true, true]|\n",
      "|       [NOR, Norway]|sensor-igauge|    [8, 9, 8]|   [true, true, true]|\n",
      "|[USA, United States]|  sensor-ipad|    [8, 8, 8]|   [true, true, true]|\n",
      "|[USA, United States]| sensor-inest|    [8, 8, 7]|   [true, true, true]|\n",
      "|[USA, United States]|  sensor-ipad|    [5, 5, 5]|   [true, true, true]|\n",
      "|        [CHN, China]|sensor-igauge|    [6, 5, 6]|   [true, true, true]|\n",
      "|[USA, United States]|  sensor-ipad|    [9, 9, 9]|   [true, true, true]|\n",
      "|[USA, United States]|sensor-igauge|    [0, 0, 0]| [false, false, fa...|\n",
      "|      [AUT, Austria]|  sensor-ipad|    [5, 5, 5]|   [true, true, true]|\n",
      "+--------------------+-------------+-------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select cca3, device_type, battery_level,\n",
    "     transform (battery_level, bl -> bl > 0) as boolean_battery_level\n",
    "     from iot_nested_data\n",
    "     \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you are not limited to only a single `transform()` function. In fact, you can chain multiple transformation, as this \n",
    "code tranforms countries to both lower and upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                cca3|          lower_cca3|          upper_cca3|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[USA, United States]|[usa, united states]|[USA, UNITED STATES]|\n",
      "|       [NOR, Norway]|       [nor, norway]|       [NOR, NORWAY]|\n",
      "|[USA, United States]|[usa, united states]|[USA, UNITED STATES]|\n",
      "|  [PHL, Philippines]|  [phl, philippines]|  [PHL, PHILIPPINES]|\n",
      "|[USA, United States]|[usa, united states]|[USA, UNITED STATES]|\n",
      "|        [CHN, China]|        [chn, china]|        [CHN, CHINA]|\n",
      "|        [JPN, Japan]|        [jpn, japan]|        [JPN, JAPAN]|\n",
      "|[USA, United States]|[usa, united states]|[USA, UNITED STATES]|\n",
      "|        [ITA, Italy]|        [ita, italy]|        [ITA, ITALY]|\n",
      "|[USA, United States]|[usa, united states]|[USA, UNITED STATES]|\n",
      "|        [IND, India]|        [ind, india]|        [IND, INDIA]|\n",
      "|       [NOR, Norway]|       [nor, norway]|       [NOR, NORWAY]|\n",
      "|[USA, United States]|[usa, united states]|[USA, UNITED STATES]|\n",
      "|[USA, United States]|[usa, united states]|[USA, UNITED STATES]|\n",
      "|[USA, United States]|[usa, united states]|[USA, UNITED STATES]|\n",
      "|        [CHN, China]|        [chn, china]|        [CHN, CHINA]|\n",
      "|[USA, United States]|[usa, united states]|[USA, UNITED STATES]|\n",
      "|[USA, United States]|[usa, united states]|[USA, UNITED STATES]|\n",
      "|      [AUT, Austria]|      [aut, austria]|      [AUT, AUSTRIA]|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select cca3,\n",
    "     transform (cca3, c -> lcase(c)) as lower_cca3,\n",
    "     transform (cca3, c -> ucase(c)) as upper_cca3\n",
    "     from iot_nested_data\n",
    "     \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use `filter()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out any devices with battery levels lower than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------------+----------+\n",
      "|                cca3|  device_type|battery_level|low_levels|\n",
      "+--------------------+-------------+-------------+----------+\n",
      "|[USA, United States]|  sensor-ipad|    [8, 9, 7]|        []|\n",
      "|       [NOR, Norway]|sensor-igauge|    [6, 6, 5]|        []|\n",
      "|[USA, United States]| sensor-inest|    [1, 1, 0]| [1, 1, 0]|\n",
      "|  [PHL, Philippines]|  sensor-ipad|    [0, 0, 0]| [0, 0, 0]|\n",
      "|[USA, United States]|sensor-istick|    [8, 8, 8]|        []|\n",
      "|        [CHN, China]|  sensor-ipad|    [9, 9, 9]|        []|\n",
      "|        [JPN, Japan]|  sensor-ipad|    [0, 0, 0]| [0, 0, 0]|\n",
      "|[USA, United States]| sensor-inest|   [9, 9, 10]|        []|\n",
      "|        [ITA, Italy]|  sensor-ipad|   [0, -1, 0]|[0, -1, 0]|\n",
      "|[USA, United States]|sensor-igauge|    [7, 7, 8]|        []|\n",
      "|        [IND, India]|  sensor-ipad|    [4, 5, 5]|       [4]|\n",
      "|       [NOR, Norway]|sensor-igauge|    [8, 9, 8]|        []|\n",
      "|[USA, United States]|  sensor-ipad|    [8, 8, 8]|        []|\n",
      "|[USA, United States]| sensor-inest|    [8, 8, 7]|        []|\n",
      "|[USA, United States]|  sensor-ipad|    [5, 5, 5]|        []|\n",
      "|        [CHN, China]|sensor-igauge|    [6, 5, 6]|        []|\n",
      "|[USA, United States]|  sensor-ipad|    [9, 9, 9]|        []|\n",
      "|[USA, United States]|sensor-igauge|    [0, 0, 0]| [0, 0, 0]|\n",
      "|      [AUT, Austria]|  sensor-ipad|    [5, 5, 5]|        []|\n",
      "+--------------------+-------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select cca3, device_type, battery_level,\n",
    "     filter (battery_level, bl -> bl < 5) as low_levels\n",
    "     from iot_nested_data\n",
    "     \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use `reduce()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" select cca3, device_type, battery_level,\n",
    "     reduce(battery_level, 0, (t, acc) -> t + acc,  acc -> acc div size(battery_level) ) as average_battery_level\n",
    "     from iot_nested_data\n",
    "     sort by average_battery_level desc\n",
    "     \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" select cca3, device_type, temp,\n",
    "     reduce(temp, 0, (t, acc) -> t + acc,  acc -> acc div size(temp) ) as average_temp\n",
    "     from iot_nested_data\n",
    "     sort by average_temp desc\n",
    "     \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" select cca3, device_type, c02_level,\n",
    "     reduce(c02_level, 0, (t, acc) -> t + acc,  acc -> acc div size(c02_level) ) as average_c02_level\n",
    "     from iot_nested_data\n",
    "     sort by average_c02_level desc\n",
    "     \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine or chain many `reduce()` functions as this code shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" select cca3, device_type, signal, temp, c02_level,\n",
    "     reduce(signal, 0, (s, sacc) -> s + sacc,  sacc -> sacc div size(signal) ) as average_signal,\n",
    "     reduce(temp, 0, (t, tacc) -> t + tacc,  tacc -> tacc div size(temp) ) as average_temp,\n",
    "     reduce(c02_level, 0, (c, cacc) -> c + cacc,  cacc -> cacc div size(c02_level) ) as average_c02_level\n",
    "     from iot_nested_data\n",
    "     sort by average_signal desc\n",
    "     \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this short tutorial has been to demonstrate the ease of utility of higher-order functions and lambda expressions in SQL, to work with JSON attributes nested structures and arrays. Once you have flattened or parsed the desired values into respective DataFrames or Datasets, and after saving them as a SQL view or table, you can as easily manipulate and tranform your arrays with higher-order functions in SQL as you would with DataFrame or Dataset API.\n",
    "\n",
    "Finally, it is easy to employ higher-order functions than to write UDFs in Python or Scala. Read the original blog on [SQL higher-order functions](https://databricks.com/blog/2017/05/24/working-with-nested-data-using-higher-order-functions-in-sql-on-databricks.html) to get more information on the _whys_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "name": "higher-order-functions-tutorial-python",
  "notebookId": 3724404464617500
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
